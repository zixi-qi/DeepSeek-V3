{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepSeek V3 Mini Training Experiments\n",
        "\n",
        "This notebook provides an interactive environment for experimenting with the mini DeepSeek V3 model training on the tiny Shakespeare dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add the mini_model directory to Python path\n",
        "sys.path.append(os.path.join(os.getcwd(), '..', 'mini_model'))\n",
        "\n",
        "from configuration_deepseek import DeepseekV3Config\n",
        "from modeling_deepseek import DeepseekV3ForCausalLM\n",
        "from data_utils import create_shakespeare_dataset, calculate_dataset_statistics\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model and Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer_path = \"../mini_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    tokenizer_path,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Add padding token if needed\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
        "\n",
        "# Load model config\n",
        "config_path = \"../mini_model/config.json\"\n",
        "with open(config_path, 'r') as f:\n",
        "    config_dict = json.load(f)\n",
        "\n",
        "config = DeepseekV3Config(**config_dict)\n",
        "\n",
        "# Initialize model\n",
        "model = DeepseekV3ForCausalLM(config)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model initialized. Total parameters: {total_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explore the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the tiny Shakespeare dataset\n",
        "dataset = load_dataset(\"karpathy/tiny_shakespeare\")\n",
        "\n",
        "print(\"Dataset structure:\")\n",
        "print(dataset)\n",
        "\n",
        "# Show sample text\n",
        "print(\"\\nSample from training set:\")\n",
        "print(dataset['train']['text'][0][:500] + \"...\")\n",
        "\n",
        "# Dataset statistics\n",
        "train_text = \"\\n\".join(dataset['train']['text'])\n",
        "val_text = \"\\n\".join(dataset['validation']['text'])\n",
        "test_text = \"\\n\".join(dataset['test']['text'])\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"Train: {len(train_text):,} characters\")\n",
        "print(f\"Validation: {len(val_text):,} characters\")\n",
        "print(f\"Test: {len(test_text):,} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Tokenization and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create custom dataset\n",
        "block_size = 128\n",
        "train_dataset = create_shakespeare_dataset(\n",
        "    tokenizer=tokenizer,\n",
        "    split=\"train\",\n",
        "    block_size=block_size,\n",
        ")\n",
        "\n",
        "# Get dataset statistics\n",
        "stats = calculate_dataset_statistics(train_dataset, tokenizer)\n",
        "\n",
        "# Test a single sample\n",
        "sample = train_dataset[0]\n",
        "print(f\"Sample input shape: {sample['input_ids'].shape}\")\n",
        "print(f\"Sample labels shape: {sample['labels'].shape}\")\n",
        "\n",
        "# Decode to see what the model will learn\n",
        "print(\"\\nDecoded sample input:\")\n",
        "print(tokenizer.decode(sample['input_ids'][:50]) + \"...\")\n",
        "print(\"\\nDecoded sample labels (shifted by 1):\")\n",
        "print(tokenizer.decode(sample['labels'][:50]) + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Training Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick training test with a few batches\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create a simple dataloader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: {\n",
        "        \"input_ids\": torch.stack([d[\"input_ids\"] for d in x]),\n",
        "        \"labels\": torch.stack([d[\"labels\"] for d in x]),\n",
        "    }\n",
        ")\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "# Move model to appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Train for a few steps\n",
        "model.train()\n",
        "losses = []\n",
        "\n",
        "print(\"Running quick training test...\")\n",
        "for i, batch in enumerate(train_loader):\n",
        "    if i >= 10:  # Just 10 steps for testing\n",
        "        break\n",
        "    \n",
        "    # Move batch to device\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    \n",
        "    # Forward pass\n",
        "    outputs = model(**batch)\n",
        "    loss = outputs.loss\n",
        "    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "    print(f\"Step {i+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Quick Training Test - Loss Curve\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAverage loss: {sum(losses)/len(losses):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test text generation (note: model is barely trained)\n",
        "model.eval()\n",
        "\n",
        "prompts = [\n",
        "    \"To be or not to be\",\n",
        "    \"O Romeo, Romeo\",\n",
        "    \"All the world's a stage\",\n",
        "]\n",
        "\n",
        "print(\"Testing generation (note: model is barely trained):\\n\")\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=50,\n",
        "            temperature=0.8,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Generated: {generated}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
