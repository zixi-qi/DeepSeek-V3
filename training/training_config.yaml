# Training configuration for mini DeepSeek V3 on tiny Shakespeare

# Model arguments
model:
  model_config_path: "../mini_model/config.json"
  tokenizer_path: "../mini_model"
  init_from_scratch: true

# Data arguments  
data:
  dataset_name: "karpathy/tiny_shakespeare"
  block_size: 512
  preprocessing_num_workers: 4
  overwrite_cache: false

# Training arguments
training:
  output_dir: "./outputs"
  num_train_epochs: 10
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 4
  learning_rate: 5e-4
  weight_decay: 0.01
  warmup_ratio: 0.1
  logging_steps: 10
  save_steps: 500
  eval_steps: 100
  evaluation_strategy: "steps"
  save_total_limit: 3
  fp16: false
  bf16: true
  gradient_checkpointing: true
  seed: 42
  dataloader_num_workers: 4
  lr_scheduler_type: "cosine"
  
# Accelerate configuration
accelerate:
  mixed_precision: "bf16"
  gradient_accumulation_steps: 4
  
# Additional training parameters
additional:
  max_grad_norm: 1.0
  use_wandb: false
  wandb_project: "deepseek-v3-mini-shakespeare"
  early_stopping_patience: 5
  early_stopping_threshold: 0.01
